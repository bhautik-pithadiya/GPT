{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizer.Dataset import CustomDataset\n",
    "from tokenizer.gpt import GptTokenizer\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = './tokenizer/models/nolan/gpt.model'\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 100\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "checkpoint_steps = 500\n",
    "vocab_size = 10002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model\")\n",
    "model = GPTLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n"
     ]
    }
   ],
   "source": [
    "print('Loading Dataset')\n",
    "DATA = pd.read_csv('./data/dataset_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher Nolan \\n Early life \\n \\n Christop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nolan \\n Early life \\n \\n Christopher Edward N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n Early life \\n \\n Christopher Edward Nolan w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early life \\n \\n Christopher Edward Nolan was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>life \\n \\n Christopher Edward Nolan was born o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Christopher Nolan \\n Early life \\n \\n Christop...\n",
       "1  Nolan \\n Early life \\n \\n Christopher Edward N...\n",
       "2  \\n Early life \\n \\n Christopher Edward Nolan w...\n",
       "3  Early life \\n \\n Christopher Edward Nolan was ...\n",
       "4  life \\n \\n Christopher Edward Nolan was born o..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenzier\n"
     ]
    }
   ],
   "source": [
    "print('Loading Tokenzier')\n",
    "tokenizer = GptTokenizer()\n",
    "tokenizer.load(tokenizer_path)\n",
    "special_tokens = {\n",
    "    '<eos>' : 10000,\n",
    "    '<pad>': 10001\n",
    "}\n",
    "tokenizer.register_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token = [7622,  2966,   295,   940,   291,   306,   289,   260,   847,   295,\n",
    "         2815,    46,   293,   450,   260,  1122,   289,   260,  1511,   283,\n",
    "         2299,   260,   488,   295,  1195,  1221,    46,   708,  5299,  3390,\n",
    "         1194,   257,   270,  6270,  1148,    46,   481,  1682,   535,   416,\n",
    "          337,  1861,   289,  4840,   289,   357,  1004,   489,    44,   260,\n",
    "          488,   369,  1331,   331,   347,   971,  4046,   293,   655,   363,\n",
    "          949,   362,  3231,  1164,    44,   366,   362,  3231,  1066,  4046,\n",
    "         1277,  7831,    46,   293,   762,   363,   450,   306,   289,  1355,\n",
    "          455,  1775,    44,   366,   756,   735,  7321,   289,  2502,  1690,\n",
    "          260,   960,   498,    44,   260,  1148,   295,   442,   597,  1775,\n",
    "           46,  1547,   433,   794,    44,   293,   331,   442,  3692,   285,\n",
    "          538,   260,   342,   564,   349,    46,  1277, ]\n",
    "# 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "#         10001, 10001, 10001, 10001, 10001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ 80,  1911,   321,   295,   940,   291,   306,   289,   260,   847,\n",
    "          295,  2815,    46,   293,   450,   260,  1122,   289,   260,  1511,\n",
    "          283,  2299,   260,   488,   295,  1195,  1221,    46,   708,  5299,\n",
    "         3390,  1194,   257,   270,  6270,  1148,    46,   481,  1682,   535,\n",
    "          416,   337,  1861,   289,  4840,   289,   357,  1004,   489,    44,\n",
    "          260,   488,   369,  1331,   331,   347,   971,  4046,   293,   655,\n",
    "          363,   949,   362,  3231,  1164,    44,   366,   362,  3231,  1066,\n",
    "         4046,  1277,  7831,    46,   293,   762,   363,   450,   306,   289,\n",
    "         1355,   455,  1775,    44,   366,   756,   735,  7321,   289,  2502,\n",
    "         1690,   260,   960,   498,    44,   260,  1148,   295,   442,   597,\n",
    "         1775,    46,  1547,   433,   794,    44,   293,   331,   442,  3692,\n",
    "          285,   538,   260,   342,   564,   349,    46,  1277,   811, ]\n",
    "        #   10001,\n",
    "        # 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "        # 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "        # 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "        # 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "        # 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "        # 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
    "        # 10001, 10001, 10001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Guy Pearce is super in it and the acting is wonderful. I like the idea and the concept of telling the story is pretty original. But wow!!! What a bummer ending. It works up all this suspense and intrigue and you find out, the story we saw was for nothing???? I don't mind not knowing everything, but not knowing anything???? Not acceptable. I didn't like it and call me stupid, but after being intrigued and following along the whole time, the ending is just too stupid. At one point, I was just ready to get the movie over with. Not\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor(input_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pearce is super in it and the acting is wonderful. I like the idea and the concept of telling the story is pretty original. But wow!!! What a bummer ending. It works up all this suspense and intrigue and you find out, the story we saw was for nothing???? I don't mind not knowing everything, but not knowing anything???? Not acceptable. I didn't like it and call me stupid, but after being intrigued and following along the whole time, the ending is just too stupid. At one point, I was just ready to get the movie over with. Not something\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_input = len(input_token)\n",
    "padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_target = len(tokens)\n",
    "padded_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 119\n"
     ]
    }
   ],
   "source": [
    "no_pad_input = len(input_token)\n",
    "no_pad_target = len(tokens)\n",
    "print(no_pad_input,no_pad_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_padding = padded_input-no_pad_input\n",
    "target_padding = padded_target - no_pad_target\n",
    "input_padding,target_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch: tensor([[ 3269,   295,   257,  2616,  6421,   347,   260, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001]])\n",
      "Target batch: tensor([[  265,   257,  2616,  6421,   347,   260,   285,  3136,   798,   266,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001]])\n",
      "Input batch: tensor([[  116,   420,  6421,   347,   260, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001]])\n",
      "Target batch: tensor([[  115,   320,   532,   347,   260,   285,  3136,   798,   266, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001, 10001,\n",
      "         10001, 10001, 10001, 10001, 10001, 10001]])\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ultraimport\n",
    "import torch\n",
    "\n",
    "GptTokenizer = ultraimport('tokenizer/gpt.py','GptTokenizer',recurse=True)\n",
    "\n",
    "tokenizer = GptTokenizer()\n",
    "tokenizer.load('./tokenizer/models/nolan/gpt.model')\n",
    "special_tokens = {\n",
    "    '<eos>': 10000,\n",
    "    '<pad>': 10001\n",
    "}\n",
    "tokenizer.register_special_tokens(special_tokens)\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]\n",
    "        input_tokens = text.Text.split(' ')\n",
    "        input_token = input_tokens[:-1]  # Leave one position for the EOS token\n",
    "        input_ids = self.tokenizer.encode(\" \".join(tokens for tokens in input_token))\n",
    "\n",
    "        # Target sequence (shifted by one)\n",
    "        target_tokens = input_tokens[1:]\n",
    "        target_ids = self.tokenizer.encode(\" \".join(tokens for tokens in target_tokens))\n",
    "\n",
    "        # Ensure input and target sequences are the same length\n",
    "        if len(input_ids) < self.max_length:\n",
    "            diff = self.max_length - len(input_ids)\n",
    "            for i in range(diff):\n",
    "                input_ids.append(special_tokens['<pad>'])\n",
    "                \n",
    "        if len(target_ids) < self.max_length:\n",
    "            diff = self.max_length - len(target_ids)\n",
    "            for i in range(diff):\n",
    "                target_ids.append(special_tokens['<pad>'])\n",
    "            \n",
    "        # while len(input_ids) != len(target_ids):\n",
    "        #     if len(input_ids) > len(target_ids):\n",
    "        #         target_ids.append(special_tokens['<pad>'])\n",
    "        #     elif len(input_ids) < len(target_ids):\n",
    "        #         input_ids.append(special_tokens['<pad>'])\n",
    "\n",
    "        input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "        target_ids_tensor = torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids_tensor, target_ids_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_batch = pad_sequence(inputs, batch_first=True, padding_value=10001)\n",
    "    target_batch = pad_sequence(targets, batch_first=True, padding_value=10001)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    data = pd.DataFrame({\"Text\": [\"This is a test sentence for the tokenizer\",'test sentence for the tokenizer']})\n",
    "    dataset = CustomDataset(data, tokenizer, max_length=256)\n",
    "    dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_batch, target_batch = batch\n",
    "        print(\"Input batch:\", input_batch)\n",
    "        print(\"Target batch:\", target_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n",
      "Custom Dataset\n",
      "DataLoader\n"
     ]
    }
   ],
   "source": [
    "print('Loading Dataset')\n",
    "DATA = pd.read_csv('./data/dataset_v2.csv')\n",
    "TRAIN_DATA,VAL_DATA = train_test_split(DATA,test_size=0.3,shuffle=True, random_state=42)\n",
    "\n",
    "print('Custom Dataset')\n",
    "train_dataset = CustomDataset(TRAIN_DATA, tokenizer, max_length=block_size)\n",
    "val_dataset = CustomDataset(VAL_DATA, tokenizer, block_size)\n",
    "\n",
    "print('DataLoader')\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size, shuffle=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312583"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "for x,y in enumerate(train_dataloader):\n",
    "    if iters == 10:\n",
    "        break\n",
    "    iters+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/dataset_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher Nolan \\n Early life \\n \\n Christop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nolan \\n Early life \\n \\n Christopher Edward N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n Early life \\n \\n Christopher Edward Nolan w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early life \\n \\n Christopher Edward Nolan was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>life \\n \\n Christopher Edward Nolan was born o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Christopher Nolan \\n Early life \\n \\n Christop...\n",
       "1  Nolan \\n Early life \\n \\n Christopher Edward N...\n",
       "2  \\n Early life \\n \\n Christopher Edward Nolan w...\n",
       "3  Early life \\n \\n Christopher Edward Nolan was ...\n",
       "4  life \\n \\n Christopher Edward Nolan was born o..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/tokenized_data_v3_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5758, 410, 2412, 551, 962, 1056, 2412, 2412, ...</td>\n",
       "      <td>[3519, 2412, 551, 962, 1056, 2412, 2412, 706, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3519, 2412, 551, 962, 1056, 2412, 2412, 706, ...</td>\n",
       "      <td>[10, 551, 962, 1056, 2412, 2412, 706, 8114, 41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10, 551, 962, 1056, 2412, 2412, 706, 8114, 41...</td>\n",
       "      <td>[69, 962, 1056, 2412, 2412, 706, 8114, 410, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[69, 962, 1056, 2412, 2412, 706, 8114, 410, 33...</td>\n",
       "      <td>[108, 1280, 2412, 2412, 706, 8114, 410, 331, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[108, 1280, 2412, 2412, 706, 8114, 410, 331, 7...</td>\n",
       "      <td>[10, 2412, 706, 8114, 410, 331, 7013, 325, 32,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  \\\n",
       "0  [5758, 410, 2412, 551, 962, 1056, 2412, 2412, ...   \n",
       "1  [3519, 2412, 551, 962, 1056, 2412, 2412, 706, ...   \n",
       "2  [10, 551, 962, 1056, 2412, 2412, 706, 8114, 41...   \n",
       "3  [69, 962, 1056, 2412, 2412, 706, 8114, 410, 33...   \n",
       "4  [108, 1280, 2412, 2412, 706, 8114, 410, 331, 7...   \n",
       "\n",
       "                                                   y  \n",
       "0  [3519, 2412, 551, 962, 1056, 2412, 2412, 706, ...  \n",
       "1  [10, 551, 962, 1056, 2412, 2412, 706, 8114, 41...  \n",
       "2  [69, 962, 1056, 2412, 2412, 706, 8114, 410, 33...  \n",
       "3  [108, 1280, 2412, 2412, 706, 8114, 410, 331, 7...  \n",
       "4  [10, 2412, 706, 8114, 410, 331, 7013, 325, 32,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   X       10 non-null     object\n",
      " 1   y       10 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df['X'] = df['X'].apply(json.loads)\n",
    "df['y'] = df['y'].apply(json.loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   X       10 non-null     object\n",
      " 1   y       10 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5758,\n",
       " 410,\n",
       " 2412,\n",
       " 551,\n",
       " 962,\n",
       " 1056,\n",
       " 2412,\n",
       " 2412,\n",
       " 706,\n",
       " 8114,\n",
       " 410,\n",
       " 331,\n",
       " 7013,\n",
       " 325,\n",
       " 32,\n",
       " 2393,\n",
       " 4089,\n",
       " 32,\n",
       " 6348,\n",
       " 48,\n",
       " 44,\n",
       " 291,\n",
       " 5430,\n",
       " 2538,\n",
       " 3790,\n",
       " 44,\n",
       " 3398,\n",
       " 46,\n",
       " 1620,\n",
       " 2077,\n",
       " 44,\n",
       " 9968,\n",
       " 269,\n",
       " 276,\n",
       " 2723,\n",
       " 410,\n",
       " 44,\n",
       " 331,\n",
       " 257,\n",
       " 2126,\n",
       " 7014,\n",
       " 2102,\n",
       " 8428,\n",
       " 283,\n",
       " 7203,\n",
       " 807,\n",
       " 8742,\n",
       " 453,\n",
       " 2413,\n",
       " 348,\n",
       " 257,\n",
       " 3434,\n",
       " 964,\n",
       " 46,\n",
       " 1620,\n",
       " 5646,\n",
       " 44,\n",
       " 611,\n",
       " 2955,\n",
       " 519,\n",
       " 1116,\n",
       " 274,\n",
       " 44,\n",
       " 331,\n",
       " 351,\n",
       " 2497,\n",
       " 7627,\n",
       " 401,\n",
       " 7851,\n",
       " 435,\n",
       " 471,\n",
       " 551,\n",
       " 118,\n",
       " 276,\n",
       " 9503,\n",
       " 44,\n",
       " 4013,\n",
       " 1128,\n",
       " 265,\n",
       " 59,\n",
       " 965,\n",
       " 534,\n",
       " 1712,\n",
       " 711,\n",
       " 348,\n",
       " 257,\n",
       " 8743,\n",
       " 283,\n",
       " 3471,\n",
       " 46,\n",
       " 645,\n",
       " 494,\n",
       " 351,\n",
       " 298,\n",
       " 355,\n",
       " 266,\n",
       " 2479,\n",
       " 44,\n",
       " 3757,\n",
       " 44,\n",
       " 289,\n",
       " 257,\n",
       " 5647,\n",
       " 2479,\n",
       " 44,\n",
       " 3854,\n",
       " 44,\n",
       " 704,\n",
       " 257,\n",
       " 3173,\n",
       " 46,\n",
       " 339,\n",
       " 1211,\n",
       " 4701,\n",
       " 567,\n",
       " 6670,\n",
       " 377,\n",
       " 719,\n",
       " 332,\n",
       " 302,\n",
       " 291,\n",
       " 7628,\n",
       " 9105,\n",
       " 289,\n",
       " 534,\n",
       " 3516,\n",
       " 702,\n",
       " 4260,\n",
       " 395,\n",
       " 291,\n",
       " 551,\n",
       " 118,\n",
       " 276,\n",
       " 9503,\n",
       " 46,\n",
       " 410,\n",
       " 5164,\n",
       " 1184,\n",
       " 8744,\n",
       " 289,\n",
       " 3855,\n",
       " 6825,\n",
       " 1671,\n",
       " 463,\n",
       " 2412,\n",
       " 500,\n",
       " 1873,\n",
       " 273,\n",
       " 535,\n",
       " 44,\n",
       " 410,\n",
       " 331]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['X'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(df['X'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [5758, 410, 2412, 551, 962, 1056, 2412, 2412, ...\n",
       "1    [3519, 2412, 551, 962, 1056, 2412, 2412, 706, ...\n",
       "2    [10, 551, 962, 1056, 2412, 2412, 706, 8114, 41...\n",
       "3    [69, 962, 1056, 2412, 2412, 706, 8114, 410, 33...\n",
       "4    [108, 1280, 2412, 2412, 706, 8114, 410, 331, 7...\n",
       "Name: X, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57580, 4100, 24120, 5510, 9620, 10560, 24120, 24120, 7060, 81140]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x*10 for x in series[0][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file has been created: temp/MGNREGA_Survey_Responses.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
