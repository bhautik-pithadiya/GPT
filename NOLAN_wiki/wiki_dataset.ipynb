{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('MyProjectName (bhautikpithadiya12@gmail.com)', 'en', timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pages = ['Christopher_Nolan',\n",
    "                'Following',\n",
    "                'Memento',\n",
    "                'Insomnia',\n",
    "                'Batman Begins',\n",
    "                'The Prestige',\n",
    "                'The Dark Knight',\n",
    "                'Inception',\n",
    "                'The Dark Knight Rises',\n",
    "                'Interstellar',\n",
    "                'Dunkirk',\n",
    "                'Tenet',\n",
    "                'Oppenheimer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_sections_text(page):\n",
    "    ignore_sections = [\"References\", \"See also\", \"External links\", \"Further reading\", \"Sources\"]\n",
    "    wiki_page = wiki_wiki.page(page)\n",
    "    \n",
    "    # Get all the sections text\n",
    "    page_sections = [x.text for x in wiki_page.sections if x.title not in ignore_sections and x.text != \"\"]\n",
    "    section_titles = [x.title for x in wiki_page.sections if x.title not in ignore_sections and x.text != \"\"]\n",
    "    \n",
    "    # Add the summary page\n",
    "    page_sections.append(wiki_page.summary)\n",
    "    section_titles.append(\"Summary\")\n",
    "\n",
    "    return page_sections, section_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_txt(pages):\n",
    "    dataset = ''\n",
    "    for page in tqdm(pages):\n",
    "        if '_' in page:\n",
    "            page = page.replace(\"_\",\" \")\n",
    "        sections, title = get_wiki_sections_text(page)\n",
    "        for section,title in zip(sections,title):\n",
    "            if '\\n' in section:\n",
    "                section = section.replace(\"\\n\",\"\")\n",
    "            # dataset += f\"Page : {page}, Section Title : {title}, Content : {section} \\n \"\n",
    "            with open('wiki_dataset.txt','a') as f:\n",
    "                f.write(f\"Page : {page}, Section Title : {title}, Content : {section}\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a54659e726846ef954d5b5874b596ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_to_txt(train_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_df(pages):\n",
    "    page_section_texts = []\n",
    "    for page in tqdm(pages):\n",
    "        if '_' in page:\n",
    "            page = page.replace(\"_\",\" \")\n",
    "        sections, titles = get_wiki_sections_text(page)\n",
    "        for section, title in zip(sections, titles):\n",
    "            page_section_texts.append({\n",
    "                'page': page,\n",
    "                'section_title': title,\n",
    "                'text': section\n",
    "            })\n",
    "    print(len(page_section_texts))\n",
    "    return pd.DataFrame(page_section_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4704a48b0c4fb8bb0cd7de296a44e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_pages_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_pages_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_pages_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morignal_train_pages.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_pages_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m, in \u001b[0;36mget_pages_df\u001b[0;34m(pages)\u001b[0m\n\u001b[1;32m      2\u001b[0m page_section_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m tqdm(pages):\n\u001b[0;32m----> 4\u001b[0m     sections, titles \u001b[38;5;241m=\u001b[39m \u001b[43mget_wiki_sections_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m section, title \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sections, titles):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m section:\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mget_wiki_sections_text\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m      3\u001b[0m wiki_page \u001b[38;5;241m=\u001b[39m wiki_wiki\u001b[38;5;241m.\u001b[39mpage(page)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get all the sections text\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m page_sections \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwiki_page\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msections\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_sections \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m section_titles \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wiki_page\u001b[38;5;241m.\u001b[39msections \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_sections \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Add the summary page\u001b[39;00m\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/wikipediaapi/__init__.py:931\u001b[0m, in \u001b[0;36mWikipediaPage.sections\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03mReturns all sections of the curent page.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m:return: List of :class:`WikipediaPageSection`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_called[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracts\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_section\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/wikipediaapi/__init__.py:1064\u001b[0m, in \u001b[0;36mWikipediaPage._fetch\u001b[0;34m(self, call)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, call) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipediaPage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetches some data?.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwiki\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_called[call] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/wikipediaapi/__init__.py:297\u001b[0m, in \u001b[0;36mWikipedia.extracts\u001b[0;34m(self, page, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m used_params \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    295\u001b[0m used_params\u001b[38;5;241m.\u001b[39mupdate(params)\n\u001b[0;32m--> 297\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_attributes(raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m], page)\n\u001b[1;32m    299\u001b[0m pages \u001b[38;5;241m=\u001b[39m raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/wikipediaapi/__init__.py:528\u001b[0m, in \u001b[0;36mWikipedia._query\u001b[0;34m(self, page, params)\u001b[0m\n\u001b[1;32m    526\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredirects\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 528\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Bhautik/GPT/.venv/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_pages_df = get_pages_df(train_pages)\n",
    "train_pages_df.to_csv(\"orignal_train_pages.csv\", index=False)\n",
    "print(train_pages_df.shape)\n",
    "train_pages_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Christopher_Nolan', 'Following', 'Memento', 'Insomnia',\n",
       "       'Batman Begins', 'The Prestige', 'The Dark Knight', 'Inception',\n",
       "       'The Dark Knight Rises', 'Interstellar', 'Dunkirk', 'Tenet',\n",
       "       'Oppenheimer'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pages_df.page.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Early life', 'Personal life and public image', 'Filmmaking style',\n",
       "       'Recognition', 'Awards and honours', 'Summary', 'Plot',\n",
       "       'Production', 'Release', 'Film and television', 'Music', 'Other',\n",
       "       'Signs and symptoms', 'Causes', 'Mechanism', 'Diagnosis',\n",
       "       'Prevention', 'Management', 'Prognosis', 'Epidemiology',\n",
       "       'Society and culture', 'Cast', 'Impact', 'Themes',\n",
       "       'Critical reception', 'Awards and nominations',\n",
       "       '2006 film adaptation', 'Sequel', 'Accolades',\n",
       "       'In popular culture', 'Marketing', 'Space', 'Organizations',\n",
       "       'Etymology and language use', 'Population', 'Politics',\n",
       "       'Administration', 'Economy', 'Cuisine', 'Prototype metre',\n",
       "       'Tourist attractions', 'Transport', 'Sports', 'Notable residents',\n",
       "       'Climate', 'Media', 'Other uses', 'Private and political life',\n",
       "       'Postwar activities', 'Final years', 'Death', 'Legacy',\n",
       "       'Publications'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pages_df.section_title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_pages_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['page','section_title','text']\n",
    "dataset = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>section_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [page, section_title, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_sliding_windows(df):\n",
    "    columns = ['page','section_title','text']\n",
    "    dataset = pd.DataFrame(columns=columns)\n",
    "    length_of_sliding_window = 256\n",
    "    page = df.page\n",
    "    section_title = df.section_title\n",
    "    text = df.text\n",
    "    text = text.split()\n",
    "    \n",
    "    total_number_of_windows = len(text)//length_of_sliding_window\n",
    "    \n",
    "    start = 0\n",
    "    end = length_of_sliding_window\n",
    "    \n",
    "    for i in range(total_number_of_windows+1):\n",
    "        if i==total_number_of_windows:\n",
    "            sliding_window = text[start:]\n",
    "        else:\n",
    "            sliding_window = text[start:end]\n",
    "        row = {\n",
    "            'page':[page],\n",
    "            'section_title' : [section_title],\n",
    "            'text':[' '.join(s for s in sliding_window)]\n",
    "        }\n",
    "        start+=length_of_sliding_window\n",
    "        end +=length_of_sliding_window\n",
    "        \n",
    "        data = pd.DataFrame(row)\n",
    "        \n",
    "        dataset = pd.concat([dataset,data],ignore_index=True)\n",
    "    \n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    dataset = pd.concat([dataset,creating_sliding_windows(df.iloc[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>section_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Christopher Edward Nolan was born on 30 July 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Belic. Nolan and Roko co-directed the surreal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Personal life and public image</td>\n",
       "      <td>Nolan is married to Emma Thomas, whom he met a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Filmmaking style</td>\n",
       "      <td>Nolan's films are largely centred in metaphysi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Filmmaking style</td>\n",
       "      <td>Bordwell, a film theorist, wrote that Nolan ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>based on American Prometheus, Oppenheimer is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>numbers for technological, and organizational,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Publications</td>\n",
       "      <td>Oppenheimer, J. Robert (1954). Science and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>J. Robert Oppenheimer (born Julius Robert Oppe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>of nuclear weapons in an armed conflict. In 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 page                   section_title  \\\n",
       "0   Christopher_Nolan                      Early life   \n",
       "1   Christopher_Nolan                      Early life   \n",
       "0   Christopher_Nolan  Personal life and public image   \n",
       "0   Christopher_Nolan                Filmmaking style   \n",
       "1   Christopher_Nolan                Filmmaking style   \n",
       "..                ...                             ...   \n",
       "3         Oppenheimer                          Legacy   \n",
       "4         Oppenheimer                          Legacy   \n",
       "0         Oppenheimer                    Publications   \n",
       "0         Oppenheimer                         Summary   \n",
       "1         Oppenheimer                         Summary   \n",
       "\n",
       "                                                 text  \n",
       "0   Christopher Edward Nolan was born on 30 July 1...  \n",
       "1   Belic. Nolan and Roko co-directed the surreal ...  \n",
       "0   Nolan is married to Emma Thomas, whom he met a...  \n",
       "0   Nolan's films are largely centred in metaphysi...  \n",
       "1   Bordwell, a film theorist, wrote that Nolan ha...  \n",
       "..                                                ...  \n",
       "3   based on American Prometheus, Oppenheimer is p...  \n",
       "4   numbers for technological, and organizational,...  \n",
       "0   Oppenheimer, J. Robert (1954). Science and the...  \n",
       "0   J. Robert Oppenheimer (born Julius Robert Oppe...  \n",
       "1   of nuclear weapons in an armed conflict. In 19...  \n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>section_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Christopher Edward Nolan was born on 30 July 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Belic. Nolan and Roko co-directed the surreal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Personal life and public image</td>\n",
       "      <td>Nolan is married to Emma Thomas, whom he met a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Filmmaking style</td>\n",
       "      <td>Nolan's films are largely centred in metaphysi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Filmmaking style</td>\n",
       "      <td>Bordwell, a film theorist, wrote that Nolan ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>based on American Prometheus, Oppenheimer is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>numbers for technological, and organizational,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Publications</td>\n",
       "      <td>Oppenheimer, J. Robert (1954). Science and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>J. Robert Oppenheimer (born Julius Robert Oppe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>of nuclear weapons in an armed conflict. In 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  page                   section_title  \\\n",
       "0    Christopher_Nolan                      Early life   \n",
       "1    Christopher_Nolan                      Early life   \n",
       "2    Christopher_Nolan  Personal life and public image   \n",
       "3    Christopher_Nolan                Filmmaking style   \n",
       "4    Christopher_Nolan                Filmmaking style   \n",
       "..                 ...                             ...   \n",
       "129        Oppenheimer                          Legacy   \n",
       "130        Oppenheimer                          Legacy   \n",
       "131        Oppenheimer                    Publications   \n",
       "132        Oppenheimer                         Summary   \n",
       "133        Oppenheimer                         Summary   \n",
       "\n",
       "                                                  text  \n",
       "0    Christopher Edward Nolan was born on 30 July 1...  \n",
       "1    Belic. Nolan and Roko co-directed the surreal ...  \n",
       "2    Nolan is married to Emma Thomas, whom he met a...  \n",
       "3    Nolan's films are largely centred in metaphysi...  \n",
       "4    Bordwell, a film theorist, wrote that Nolan ha...  \n",
       "..                                                 ...  \n",
       "129  based on American Prometheus, Oppenheimer is p...  \n",
       "130  numbers for technological, and organizational,...  \n",
       "131  Oppenheimer, J. Robert (1954). Science and the...  \n",
       "132  J. Robert Oppenheimer (born Julius Robert Oppe...  \n",
       "133  of nuclear weapons in an armed conflict. In 19...  \n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('train_pages.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_moving_sliding_windows(df):\n",
    "    columns = ['page','section_title','text']\n",
    "    dataset = pd.DataFrame(columns=columns)\n",
    "    length_of_sliding_window = 128\n",
    "    page = df.page\n",
    "    section_title = df.section_title\n",
    "    text = df.text\n",
    "    text = text.split()\n",
    "    \n",
    "    total_number_of_windows = len(text) - length_of_sliding_window + 1\n",
    "    start = 0\n",
    "    end = length_of_sliding_window\n",
    "    \n",
    "    if total_number_of_windows<0:\n",
    "        sliding_window = text[:-1]\n",
    "        row = {\n",
    "            'page':[page],\n",
    "            'section_title' : [section_title],\n",
    "            'text':[' '.join(s for s in sliding_window)]\n",
    "        }\n",
    "        data = pd.DataFrame(row)\n",
    "        \n",
    "        dataset = pd.concat([dataset,data],ignore_index=True)\n",
    "    else:    \n",
    "        while end <= len(text):\n",
    "            sliding_window = text[start:end]\n",
    "            row = {\n",
    "                'page':[page],\n",
    "                'section_title' : [section_title],\n",
    "                'text':[' '.join(s for s in sliding_window)]\n",
    "            }\n",
    "            start+=1\n",
    "            end +=1\n",
    "            \n",
    "            data = pd.DataFrame(row)\n",
    "            \n",
    "            dataset = pd.concat([dataset,data],ignore_index=True)\n",
    "    \n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['page','section_title','text']\n",
    "moving_window_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    moving_window_df = pd.concat([moving_window_df,creating_moving_sliding_windows(df.iloc[i])])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_window_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>section_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Christopher Edward Nolan was born on 30 July 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Edward Nolan was born on 30 July 1970, in West...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>Nolan was born on 30 July 1970, in Westminster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>was born on 30 July 1970, in Westminster, Lond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christopher_Nolan</td>\n",
       "      <td>Early life</td>\n",
       "      <td>born on 30 July 1970, in Westminster, London. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15472</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>the development of the hydrogen bomb during a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15473</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>development of the hydrogen bomb during a 1949...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15474</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>of the hydrogen bomb during a 1949–1950 govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15475</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>the hydrogen bomb during a 1949–1950 governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15476</th>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>Summary</td>\n",
       "      <td>hydrogen bomb during a 1949–1950 governmental ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15477 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    page section_title  \\\n",
       "0      Christopher_Nolan    Early life   \n",
       "1      Christopher_Nolan    Early life   \n",
       "2      Christopher_Nolan    Early life   \n",
       "3      Christopher_Nolan    Early life   \n",
       "4      Christopher_Nolan    Early life   \n",
       "...                  ...           ...   \n",
       "15472        Oppenheimer       Summary   \n",
       "15473        Oppenheimer       Summary   \n",
       "15474        Oppenheimer       Summary   \n",
       "15475        Oppenheimer       Summary   \n",
       "15476        Oppenheimer       Summary   \n",
       "\n",
       "                                                    text  \n",
       "0      Christopher Edward Nolan was born on 30 July 1...  \n",
       "1      Edward Nolan was born on 30 July 1970, in West...  \n",
       "2      Nolan was born on 30 July 1970, in Westminster...  \n",
       "3      was born on 30 July 1970, in Westminster, Lond...  \n",
       "4      born on 30 July 1970, in Westminster, London. ...  \n",
       "...                                                  ...  \n",
       "15472  the development of the hydrogen bomb during a ...  \n",
       "15473  development of the hydrogen bomb during a 1949...  \n",
       "15474  of the hydrogen bomb during a 1949–1950 govern...  \n",
       "15475  the hydrogen bomb during a 1949–1950 governmen...  \n",
       "15476  hydrogen bomb during a 1949–1950 governmental ...  \n",
       "\n",
       "[15477 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_window_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_window_df.to_csv('moving_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-kt4yiry4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-kt4yiry4\n",
      "  Resolved https://github.com/huggingface/transformers to commit 73014b561d5f88d728e46a57d346f516fefe3f2d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0.dev0)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from transformers==4.41.0.dev0) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.41.0.dev0) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.41.0.dev0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ksuser/Bhautik/GPT/.venv/lib/python3.10/site-packages (from requests->transformers==4.41.0.dev0) (2024.2.2)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.41.0.dev0-py3-none-any.whl size=9044069 sha256=7f7a9f94c379d2dcf67c33d22a7da521083634f16007dd71f4d64a630786dcaa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gnwqwnpi/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.39.3\n",
      "    Uninstalling transformers-4.39.3:\n",
      "      Successfully uninstalled transformers-4.39.3\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.41.0.dev0\n",
      "tokenizers               0.19.1\n",
      "transformers             4.41.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nolan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
